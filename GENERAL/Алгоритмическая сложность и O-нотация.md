#algorithms #complexity #dataStructures #bigONotation #computerScience

# Алгоритмическая сложность и O-нотация

```table-of-contents
```

## Что такое алгоритмическая сложность

Алгоритмическая сложность — это математическая характеристика, которая описывает, как изменяется время выполнения алгоритма или потребление ресурсов (память) в зависимости от размера входных данных. Сложность алгоритма не измеряет абсолютное время выполнения в секундах, а скорее описывает скорость роста этого времени относительно увеличения объема входных данных.

При анализе алгоритмов мы используем асимптотическую нотацию (O-нотацию), чтобы абстрагироваться от конкретных деталей реализации и сосредоточиться на общем поведении алгоритма при больших объемах данных.

## Основные типы асимптотической нотации

В теории алгоритмов используются три основных типа асимптотической нотации:

- **O-нотация (big-O)** — верхняя граница, описывает худший случай
- **Ω-нотация (big-Omega)** — нижняя граница, описывает лучший случай
- **Θ-нотация (big-Theta)** — точная граница, описывает средний случай

Чаще всего на практике используется именно O-нотация, так как она позволяет оценить наихудшее поведение алгоритма, что критично для систем реального времени и других приложений, где важна предсказуемость.

## Линейная сложность O(n)

Линейная сложность O(n) означает, что время выполнения алгоритма растет пропорционально размеру входных данных. Если размер входных данных увеличивается в два раза, то и время выполнения также увеличивается примерно в два раза.

Типичный пример алгоритма с линейной сложностью — простой проход по массиву:

```go
func linearSearch(arr []int, target int) int {
    for i, val := range arr {
        if val == target {
            return i
        }
    }
    return -1 // элемент не найден
}
```

Даже если мы найдем искомый элемент в начале массива и сразу завершим выполнение функции, сложность все равно считается O(n), потому что в худшем случае (элемент в конце массива или отсутствует) нам придется просмотреть все n элементов.

## Константная сложность O(1)

Константная сложность O(1) означает, что время выполнения алгоритма не зависит от размера входных данных. Такие алгоритмы выполняются за фиксированное количество операций независимо от объема данных.

Примеры операций с константной сложностью:
- Доступ к элементу массива по индексу
- Добавление элемента в стек или очередь
- Получение первого или последнего элемента связного списка

```go
func getElement(arr []int, index int) int {
    return arr[index]
}
```

## Квадратичная сложность O(n²)

Квадратичная сложность возникает, когда для каждого элемента входных данных мы выполняем O(n) операций. Типичный пример — вложенные циклы:

```go
func bubbleSort(arr []int) {
    n := len(arr)
    for i := 0; i < n; i++ {
        for j := 0; j < n-i-1; j++ {
            if arr[j] > arr[j+1] {
                arr[j], arr[j+1] = arr[j+1], arr[j]
            }
        }
    }
}
```

Алгоритмы с квадратичной сложностью становятся очень медленными при больших объемах данных. Сортировка пузырьком, вставками и выбором — все имеют квадратичную сложность в худшем случае.

## Логарифмическая сложность O(log n)

Логарифмическая сложность O(log n) возникает, когда алгоритм уменьшает проблему примерно вдвое на каждом шаге. Это очень эффективная сложность, потому что даже для очень больших объемов данных log(n) остается относительно небольшим числом.

Классический пример — бинарный поиск в отсортированном массиве:

```go
func binarySearch(arr []int, target int) int {
    left, right := 0, len(arr)-1
    
    for left <= right {
        mid := left + (right-left)/2
        
        if arr[mid] == target {
            return mid
        }
        
        if arr[mid] < target {
            left = mid + 1
        } else {
            right = mid - 1
        }
    }
    
    return -1 // элемент не найден
}
```

Бинарный поиск работает за O(log n), потому что на каждом шаге мы отбрасываем половину оставшихся элементов. Другие примеры алгоритмов с логарифмической сложностью включают операции в сбалансированных бинарных деревьях поиска (AVL, красно-черные деревья).

## Линейно-логарифмическая сложность O(n log n)

Сложность O(n log n) часто встречается в эффективных алгоритмах сортировки, таких как быстрая сортировка (QuickSort), сортировка слиянием (MergeSort) и сортировка кучей (HeapSort).

Эта сложность возникает, когда для каждого из n элементов выполняется операция с логарифмической сложностью, или когда выполняется log(n) операций, каждая из которых имеет линейную сложность.

```go
func mergeSort(arr []int) []int {
    if len(arr) <= 1 {
        return arr
    }
    
    mid := len(arr) / 2
    left := mergeSort(arr[:mid])
    right := mergeSort(arr[mid:])
    
    return merge(left, right)
}

func merge(left, right []int) []int {
    result := make([]int, 0, len(left)+len(right))
    i, j := 0, 0
    
    for i < len(left) && j < len(right) {
        if left[i] <= right[j] {
            result = append(result, left[i])
            i++
        } else {
            result = append(result, right[j])
            j++
        }
    }
    
    result = append(result, left[i:]...)
    result = append(result, right[j:]...)
    
    return result
}
```

Сортировка слиянием разделяет массив на две половины (что дает log n уровней рекурсии), а затем выполняет операцию слияния с линейной сложностью на каждом уровне, что в итоге дает O(n log n).

## Экспоненциальная сложность O(2ⁿ) и факториальная сложность O(n!)

Алгоритмы с экспоненциальной сложностью O(2ⁿ) и факториальной сложностью O(n!) становятся неприменимыми даже для относительно небольших объемов данных.

Пример алгоритма с экспоненциальной сложностью — наивное рекурсивное вычисление чисел Фибоначчи:

```go
func fibonacci(n int) int {
    if n <= 1 {
        return n
    }
    return fibonacci(n-1) + fibonacci(n-2)
}
```

Факториальная сложность O(n!) встречается в алгоритмах перебора всех возможных перестановок, например, в задаче коммивояжера при полном переборе:

```go
func permutations(arr []int) [][]int {
    var result [][]int
    generatePermutations(arr, 0, &result)
    return result
}

func generatePermutations(arr []int, start int, result *[][]int) {
    if start == len(arr) - 1 {
        temp := make([]int, len(arr))
        copy(temp, arr)
        *result = append(*result, temp)
        return
    }
    
    for i := start; i < len(arr); i++ {
        arr[start], arr[i] = arr[i], arr[start]
        generatePermutations(arr, start+1, result)
        arr[start], arr[i] = arr[i], arr[start] // возвращаем обратно
    }
}
```

Важно отметить, что O(2ⁿ) и O(n²) — это совершенно разные сложности. Квадратичная сложность O(n²) растет намного медленнее, чем экспоненциальная O(2ⁿ).

## Амортизированная сложность

Амортизированная сложность учитывает среднюю производительность операции за длительный период времени. Например, при добавлении элемента в динамический массив (slice в Go) обычно сложность составляет O(1), но иногда требуется перераспределение памяти, что имеет сложность O(n).

```go
func append(slice []int, elements ...int) []int {
    // Если есть место в уже выделенной памяти - O(1)
    // Если нужно перераспределение - O(n)
    return append(slice, elements...)
}
```

В Go при добавлении элемента в slice, когда емкость исчерпана, создается новый массив с большей емкостью (обычно в 2 раза больше), и все элементы копируются туда. Это операция O(n), но она происходит редко, поэтому амортизированная сложность добавления элемента остается O(1).

## Сравнение различных сложностей

![Сравнение алгоритмических сложностей](/assets/algo.png)

Как видно из графика, алгоритмы с разной сложностью демонстрируют принципиально разное поведение при увеличении объема входных данных:

- O(1) — константное время, не зависит от размера входных данных
- O(log n) — растет очень медленно, эффективно для больших объемов данных
- O(n) — линейный рост, приемлемо для большинства задач
- O(n log n) — немного быстрее растет, чем линейно, но все еще эффективно
- O(n²) — квадратичный рост, становится проблемой для больших данных
- O(2ⁿ), O(n!) — экспоненциальный и факториальный рост, практически неприменимы для больших объемов данных

## Практические рекомендации

При разработке алгоритмов следует стремиться к минимальной возможной сложности. Вот некоторые рекомендации:

1. Для поиска в неотсортированных данных обычно требуется O(n), но в отсортированных можно достичь O(log n)
2. Избегайте вложенных циклов без необходимости, они часто приводят к квадратичной сложности
3. Используйте хеш-таблицы (map в Go) для достижения константного времени доступа O(1)
4. Для сортировки предпочитайте алгоритмы с O(n log n) вместо O(n²)
5. Динамическое программирование часто помогает избежать экспоненциальной сложности в рекурсивных алгоритмах

Понимание алгоритмической сложности — ключевой навык для разработки эффективных программ, особенно для систем, работающих с большими объемами данных.

>[!quote] Старая версия
```
	## Про сложность алгоритма O(n)
	
	Сложность алгоритма это величина, которая измеряет количество инструкций процессора затраченное на выполнение алгоритма.
	
	Если есть цикл пробегающий по всем элементам массива из n элементов, то это значит что процессору потребуется выполнить n инструкций, следовательно сложность будет O(n).
	
	При оценке сложности всегда считается худший случай, то есть, если мы выходим из цикла при нахождении ответа раньше, то сложность всё равно будет O(n), так как в худшем случае ответ будет в последнем ​элементе массива.
	
	Из этого следует что сложность алгоритма с двумя вложенными циклами квадратична, то есть O(n^2) и т.д.
	
	Если алгоритм выполняется за строго ограниченное количество инструкций (например обращение к элементу массива по индексу), то такая сложность называется константной O(1).
	
	​Есть ещё факториальная сложность O(n!) такая сложность очень плоха, и алгоритмы вообще не эффективны. Также как и сложность O(2^n). Пример такого алгоритма - рекурсивное вычисление чисел Фиббаначи (вариант без мемоизации).
	
	Замечание: O(n^2) не равно O(2^n). Временная сложность O(n^2) означает, что время выполнения алгоритма увеличивается квадратично с увеличением размера входных данных, тогда как O(2^n) означает экспоненциальную временную сложность, при которой время выполнения алгоритма увеличивается экспоненциально с увеличением размера входных данных.
	
	Стоит оговориться, если нас просят оценить, например, сложность добавления элемента в массив, то тут будет сложность O(1\*), так называемая амортизированная сложность. То есть добавляем элемент всегда за O(1), но когда нам нужно запросить новую память для элемента при расширении создастся новый массив и туда всё скопируется. Сложность будет O(n).
	
	![](./assets/algo.png)
	
	Пояснения: O – оценка для худшего случая, Ω – оценка для лучшего случая, Θ – оценка для среднего случая.
	
	## Про O(log n)
	
	O(log n) - это алгоритмическая сложность, которая означает, что время выполнения алгоритма увеличивается не пропорционально размеру входных данных (n), а по логарифмической шкале. То есть, при увеличении n в 10 раз, время выполнения алгоритма увеличится только на несколько единиц.
	
	Это очень эффективная сложность, и она часто встречается в алгоритмах, которые работают с отсортированными данными, такими как бинарный поиск. В бинарном поиске мы делим отсортированный массив пополам на каждом шаге, что дает нам O(log n) времени выполнения.
	
	Однако, не все алгоритмы могут работать с O(log n) сложностью. Например, сортировка пузырьком имеет O(n^2) сложность, что означает, что время выполнения алгоритма увеличивается пропорционально квадрату размера входных данных. Поэтому важно выбирать подходящий алгоритм в зависимости от задачи, которую необходимо решить.
	
	## Про O(n log n)
	
	O(n \* log(n)) означает, что время выполнения алгоритма увеличивается в логарифмической пропорции с увеличением размера входных данных. Это временная сложность, которая является более быстрой, чем квадратичная O(n^2), но медленнее, чем линейная O(n) или константная O(1). Алгоритмы с временной сложностью O(n log n) часто используются в сортировке данных, например, быстрой сортировке (quicksort) или сортировке слиянием (merge sort).
```
